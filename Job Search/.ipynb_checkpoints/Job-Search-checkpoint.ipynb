{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "#!pip install requests -q\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.corpus import stopwords\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create doc of search cities once finalized\n",
    "locations = [\n",
    "    'Pittsburgh',  'Nashville',  'California',  'Austin',  'Minneapolis',  'San Antonio', \n",
    "    'Indianapolis', 'TN', 'MN', 'PA', 'IN', 'CA'\n",
    "]\n",
    "locationDict = {locations.index(l): l for l in locations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description corpus i.e. each description is a document\n",
    "#alltext = desc\n",
    "sk = ['python', 'pyspark', 'numpy', 'pandas', 'nltk', 'gensim', 'scikit-learn', 'sklearn', 'nlp', 'tensorflow',\n",
    "     'machine learning', 'AI', 'BI', 'ML', 'analysis', 'excel', 'vba', 'C#', 'Java', 'STATA', \n",
    "      'EVIEWS', 'deep learning', 'master\\'s', 'masters', 'bachelor\\'s', 'bachelors', 'models']\n",
    "\n",
    "immigration = ['immigration', 'W2', 'sponsorship', 'visa', 'citizenship status']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Scientist\n",
    "Data Analyst\n",
    "BI Developer\n",
    "Decision Science Analyst\n",
    "Machine Learning\n",
    "Applied Scientist\n",
    "Modeling Analyst\n",
    "Business Analyst\n",
    "Business Intelligence Analyst\n",
    "Business Intelligence Developer\n",
    "Quantitative Analyst\n",
    "Artificial Intelligence\n",
    "Big Data \n",
    "Data Engineer\n",
    "Software Engineer\n",
    "Applied Scientist\n",
    "\n",
    "#for later\n",
    "#attributes\n",
    "job_link = ''\n",
    "job_title = ''\n",
    "company_name = ''\n",
    "job_location = ''\n",
    "description = ''\n",
    "tech_skills = ''\n",
    "immigration_assistance = False\n",
    "relocation_assistance = False\n",
    "daily_postings_ct = [] #total daily postings\n",
    "new_daily_titles = [] #new daily titles\n",
    "\n",
    "self._job_title = kwargs['job']\n",
    "self._company_name = kwargs['name']\n",
    "self._job_location = kwargs['loc']\n",
    "self._description = kwargs['desc']\n",
    "self._tech_skills = []\n",
    "self._immigration_assistance = False\n",
    "self._relocation_assistance = False\n",
    "\n",
    "        \n",
    "        self._job_title = job_title\n",
    "        self._company_name = company_name\n",
    "        self._job_location = job_location\n",
    "        self._description = description\n",
    "        self._tech_skills = tech_skills\n",
    "        self._immigration = immigration\n",
    "        self._relocation = relocation\n",
    "        \n",
    "        self.tech_skills(desc.get_text())\n",
    "                self.allows_immigrants(desc.get_text())\n",
    "                self.has_relocation_help(desc.get_text())\n",
    "            #add title to running list if it is not there already\n",
    "            self.update_titles_list(self.job_title)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for each job posting\n",
    "class Posting:        \n",
    "    def __init__(self, link):\n",
    "        self.job_link = link\n",
    "        #get details and assign to attributes\n",
    "        details = self.get_job_posting_info()\n",
    "        self.job_title = details.get('title')\n",
    "        self.company_name = details.get('company')\n",
    "        self.job_location = details.get('location')\n",
    "        self.description = details.get('desc')\n",
    "        self.tech_skills = []\n",
    "        self.immigration_assistance = False\n",
    "        self.relocation_assistance = False\n",
    "    \n",
    "    def job_title(self, job=None):\n",
    "        if job: self._job.title = job\n",
    "        return self._job.title\n",
    "    \n",
    "    #extract desired skills from description\n",
    "    def tech_skills(self, desc = None):\n",
    "        #model, work in progress. Will move here once finalized for phase 1 \n",
    "        if desc: #do stuff\n",
    "            self._tech_skills = desc\n",
    "        #return self._tech_skills\n",
    "    \n",
    "    #extract citizenship stance of organization or for position from description\n",
    "    def allows_immigrants(self, desc = None):\n",
    "        #model, work in progress. Will move here once finalized for phase 2        \n",
    "        #flag = True #do stuff here\n",
    "        self._immigration_assistance = flag\n",
    "        return self._immigration_assistance\n",
    "    \n",
    "    #extract relocation assistance stance of organization or for position from description\n",
    "    def has_relocation_help(self, desc = None):\n",
    "        #model, work in progress. Will move here once finalized for phase 2        \n",
    "        #flag = True #do stuff here\n",
    "        self._relocation_assistance = flag\n",
    "        return self._relocation_assistance\n",
    "\n",
    "    #method to get the info in the links in order\n",
    "    def get_job_posting_info(self):      \n",
    "        #get page data\n",
    "        page = requests.get(self.job_link)\n",
    "        soup = bs(page.content, 'lxml')\n",
    "        #get value fields\n",
    "        job = soup.find('h1', 'topcard__title')\n",
    "        location = soup.find('span', 'topcard__flavor topcard__flavor--bullet')\n",
    "        company = soup.find('a', 'topcard__org-name-link topcard__flavor--black-link')\n",
    "        desc = soup.find('div', 'description__text description__text--rich')\n",
    "        #check to see for desired locations and data is available for extraction\n",
    "        loc_list = ' '.join(locationDict.values())\n",
    "        if job is not None:\n",
    "            #add job attributes if verified\n",
    "            job = job.get_text().strip()\n",
    "            if location is not None: location = location.get_text().strip()\n",
    "            if company is not None: company = company.get_text().strip()\n",
    "            #check description and parse for skills, immigration, and relo criteria\n",
    "            if desc is not None: desc = desc.get_text()\n",
    "            details = {'title': job, 'location': location, 'company': company, 'desc': desc}\n",
    "            return details  \n",
    "        else: self = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data Scientist'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link = 'https://www.linkedin.com/jobs/view/data-scientist-at-wish-1881573563?refId=c8161b02-9e76-41b6-aa27-26fdca101ea6&position=3&pageNum=0&trk=public_jobs_job-result-card_result-card_full-click'\n",
    "l = 'https://www.linkedin.com/jobs/view/data-scientist-at-warner-bros-entertainment-1875477451?refId=c8161b02-9e76-41b6-aa27-26fdca101ea6&position=1&pageNum=0&trk=public_jobs_job-result-card_result-card_full-click'\n",
    "new = Posting(link=l)\n",
    "new.job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for adding newer titles to titles list if not yet available/update new titles count\n",
    "def update_titles_list(titles, postings):\n",
    "    day_titles = [] #new titles today\n",
    "    for posting in postings:\n",
    "        results = [] #whether or not title is new\n",
    "        new_t = posting.job_title.lower().strip() #job_title from search result\n",
    "        for t in titles:\n",
    "            title = t.lower().strip()\n",
    "            #if both =-1, they are not subsets of each other\n",
    "            if title.find(new_t) + new_t.find(title) == -2: results.append(True)\n",
    "            else: results.append(False)\n",
    "        if False not in results: day_titles.append(posting.job_title)\n",
    "    #update titles document for next searches\n",
    "    file = open('titles.txt', mode='a+')\n",
    "    for t in day_titles: file.write('\\n' + t)\n",
    "    file.close()\n",
    "    print('Updated total search titles: {}'.format(len(titles) + len(day_titles)))\n",
    "    print('Search titles added on {}: {}'.format(datetime.now().strftime('%m-%d-%y'), \n",
    "                                                 len(day_titles)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to get search results for dictionary positions and all locations before filtering\n",
    "#Posted in the last 24 hours and <= 10 miles from job location\n",
    "def get_all_location_results(job_titles, location): \n",
    "    end = len(job_titles)\n",
    "    links = []\n",
    "    print(end, 'search terms: \\n-----------------------------\\n')\n",
    "    for title in job_titles:\n",
    "        URL = 'https://www.linkedin.com/jobs/search?keywords='+ title +'&location='+location+'&f_TP=1'\n",
    "        page = requests.get(URL)\n",
    "        soup = bs(page.content, 'lxml')\n",
    "        refs = soup.find_all('a', class_='result-card__full-card-link')\n",
    "        links += [ref.get('href') for ref in refs if ref.get('href') not in links]\n",
    "    print(location + ':', len(links), 'result links')\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to tokenize and pre-process job descriptions\n",
    "def tokenize_job_descriptions(postings):\n",
    "    word_freq = defaultdict(int)\n",
    "    descs = []\n",
    "    for posting in postings:\n",
    "        desc = posting.tech_skills\n",
    "        if desc is not None:\n",
    "            desc = desc.lower()\n",
    "            desc = re.sub(r'[^c#\\w\\s]', ' ', desc)\n",
    "            desc = desc.split()\n",
    "            desc = [word for word in desc if word not in stopwords.words('english')]\n",
    "            desc = [word for word in desc if len(word) > 1 or (len(word) == 1 and word =='r')]\n",
    "            desc.append(posting.job_title)\n",
    "            for word in desc:\n",
    "                word_freq[word] += 1\n",
    "            descs.append(desc)\n",
    "    print('Total unique tokens from list:', len(word_freq))\n",
    "    return descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time printer\n",
    "def print_time(note, t):\n",
    "    print(note, '{m}:{s:02d} mins'\\\n",
    "          .format(m = round((time() - t )/ 60), s = round((time()-t)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26 search terms: \n",
      "-----------------------------\n",
      "\n",
      "United-States: 650 result links\n",
      "Time to get search results: 0:23 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "#get titles to search by\n",
    "titles = pd.read_csv('titles.txt', header=None)[0].values.tolist()\n",
    "\n",
    "links = []\n",
    "links += get_all_location_results(titles, 'United-States')\n",
    "print_time('Time to get search results:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to check whether title is valid for entry, associate, internship level\n",
    "def is_valid_title(new_job):\n",
    "    filters = ['VP', 'manager', 'senior', 'sr', 'president', 'vice president', 'director']\n",
    "    results = [] #boolean indicators\n",
    "    for w in filters:\n",
    "        if new_job.job_title.upper().find(w.upper()) == -1: results.append(True) #valid job\n",
    "        else: results.append(False)\n",
    "    if False in results: #job contains one of the key words in its title\n",
    "        return False\n",
    "    else: return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated total search titles: 26\n",
      "Search titles added on 05-29-20: 0\n",
      "Total job postings: 49\n",
      "Time to extract link info: 1:33 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "postings = []\n",
    "for link in links[10:60]:\n",
    "    new_job = Posting(link=link)\n",
    "    if new_job and is_valid_title(new_job) : postings.append(new_job)\n",
    "update_titles_list(titles, postings)\n",
    "print('Total job postings:', len(postings))\n",
    "print_time('Time to extract link info:', t)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "descs = tokenize_job_descriptions(postings)\n",
    "print_time('Time to tokenize values:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Links:', len(links), 'postings:', len(postings))\n",
    "print('descriptions:',len(descs))\n",
    "\n",
    "for posting in postings:\n",
    "    print(posting.job_title, posting.company_name, posting.posted_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Gensim Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim model\n",
    "#size: The number of dimensions of the embeddings and the default is 100.\n",
    "#window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "#min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "#workers: The number of partitions during training and the default workers is 3.\n",
    "#sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "model = Word2Vec(min_count=1, size= 50,workers=3, window =3, sg = 1, alpha=0.03, min_alpha=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "model.build_vocab(descs, progress_per=10000)\n",
    "print_time('Time to build vocab:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "size = int(len(descs)*0.8)\n",
    "model.train(descs[:size], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "print_time('Time to train model:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make model more memory efficient\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#what words are most similar to this\n",
    "model.wv.most_similar(positive=['python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how similar are these two\n",
    "model.wv.similarity('python', 'spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#odd one out\n",
    "model.wv.doesnt_match(['master', 'python', 'r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#master is to degree as python is to....?\n",
    "model.wv.most_similar(positive=['master', 'python'], negative=['degree'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF with sk learn trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#modules needed\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "descs = []\n",
    "#Pre processing: remove punctuation and symbols, change all to lower case, tokenize\n",
    "for d in descriptions:\n",
    "    d = d.lower()\n",
    "    d = re.sub(r'[^\\w\\s]', ' ', d)\n",
    "    descs.append(d)\n",
    "\n",
    "#Create vectorizer for the descriptions\n",
    "vect = TfidfVectorizer(max_df = 0.7, min_df = 3, stop_words = stopwords.words('english'))\n",
    "tfidf = vect.fit_transform(descs)\n",
    "#for idx in range(tfIDF[0].shape[0]):\n",
    "  #  print(vect.get_feature_names()[idx], tfIDF[0][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#(tfidf[0].toarray())\n",
    "words = list(vect.vocabulary_.keys()) #the feature names\n",
    "#use key values and keys to change the dataframe so that we can see the names of the words\n",
    "keys = list(vect.vocabulary_.values()) #the number representing the wordID in the tfidf vectors (docID, wordID)\n",
    "values = vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for word, key in values.items(): #every word\n",
    "    scores = scores.rename(columns = {key : word})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#row index = word keyID\n",
    "#column index = document ID\n",
    "scores = pd.DataFrame(tfidf.toarray())\n",
    "x = scores[3][0] #column index, row index (in that order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = scores.sum(axis=0) #all values tfidf sums\n",
    "b = scores.sum(axis=0).index #all words/index\n",
    "c = scores.sum(axis=0).values #all sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomorrow Inshallah: create small methods to parse through a string to look for substring before saving job posting:\n",
    "- Only save posting if relo assistance is offered\n",
    "- Only save posting if immigration is offered\n",
    "- Send list of job posts to excel and download/add to it\n",
    "- add link of listing to the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = 'Data Scientist - Strategic Data Solutions'\n",
    "s = 'data scientist'\n",
    "s.lower().find(ss.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.find(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.lower().find(ss.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(['data', 'scientist'])\n",
    "b = set(['data', 'UUU' ,'scientist', ])\n",
    "a.issubset(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.issubset(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
