{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "#!pip install requests -q\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for each job posting\n",
    "class Job_Posting:\n",
    "    #attributes\n",
    "    \n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def __init__(self, title, company, location, date, education, skills, visa, relo):\n",
    "        self.job_title = title\n",
    "        self.company_name = company\n",
    "        self.job_location = location\n",
    "        self.posted_date = date\n",
    "        self.education = education\n",
    "        self.tech_skills = self.get_skills_requested(skills)\n",
    "        self.immigration_assistance = visa\n",
    "        self.relocation_assistance = relo\n",
    "        \n",
    "    def get_skills_requested(self, description):\n",
    "        skills = []\n",
    "        #parse through description to extract skills if they are in your dictionary and return list\n",
    "        return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to get search results for dictionary positions and all locations before filtering\n",
    "#Posted in the last 24 hours and <= 10 miles from job location\n",
    "def get_all_location_results(job_titles, location):     \n",
    "    stime = datetime.now()\n",
    "    start = 0\n",
    "    #cut_off = 20\n",
    "    end = len(job_titles)\n",
    "    links = []\n",
    "    print(end, 'search terms: \\n-----------------------------\\n')\n",
    "    for title in job_titles[start: end]:\n",
    "        URL = 'https://www.linkedin.com/jobs/search?keywords='+ title +'&location='+location+'&f_TP=1'\n",
    "        page = requests.get(URL)\n",
    "        soup = bs(page.content, 'lxml')\n",
    "        refs = soup.find_all('a', class_='result-card__full-card-link')\n",
    "        links += [ref.get('href') for ref in refs if ref.get('href') not in links]   \n",
    "    print(location + ':', len(links), 'results')\n",
    "    etime = datetime.now()\n",
    "    mins = (etime-stime)\n",
    "    print('\\nAll results:', len(links))\n",
    "    print('Total time:', round(mins.seconds/60), 'mins:' + str(mins.seconds%60), 'secs')\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionaries of search parameters\n",
    "#Later on, add values to this as you get more job titles from search results and we can phase this out boo :)\n",
    "titles = [\n",
    "    'Data Scientist', 'Data', 'BI', 'Decision Science', 'ML', 'Machine Learning', 'Applied Scientist', \n",
    "    'Modeling Analyst', 'Business Analyst', 'Business Intelligence', 'Analyst',  'Developer', 'Scientist', \n",
    "    'Quantitative', 'Artificial Intelligence', 'AI', 'Engineer', 'Big-Data', 'Software Engineer'\n",
    "]\n",
    "size = len(titles)\n",
    "#search result job titles\n",
    "addon_titles = []\n",
    "\n",
    "qualifiers = ['Intern', 'Junior']\n",
    "for t in titles[:size]: \n",
    "    titles.extend([qualifiers[0]+'-'+ t, qualifiers[1]+'-' + t])\n",
    "    \n",
    "locationDict = {\n",
    "    0: 'Pittsburgh',  1: 'Nashville',  2: 'Charlotte',  3: 'California',  4: 'Austin',  5: 'Minneapolis',\n",
    "    6: 'San Antonio',  7: 'Massachusetts'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 search terms: \n",
      "-----------------------------\n",
      "\n",
      "United-States: 25 results\n",
      "\n",
      "All results: 25\n",
      "Total time: 0 mins:1 secs\n"
     ]
    }
   ],
   "source": [
    "#for testing\n",
    "links = get_all_location_results(titles[0:1], 'United-States')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the info in the links\n",
    "postings = []\n",
    "job_titles = []#new stuff\n",
    "for link in links:\n",
    "    page = requests.get(link)\n",
    "    soup = bs(page.content, 'lxml')\n",
    "    job = re.sub(r'[^\\w\\s]','',soup.find('h1', 'topcard__title').get_text().strip())\n",
    "    if job.title() not in titles: \n",
    "        titles.append(job.title()) \n",
    "    company = soup.find('a', 'topcard__org-name-link topcard__flavor--black-link').get_text().strip()\n",
    "    location = soup.find('span', 'topcard__flavor topcard__flavor--bullet').get_text().strip()\n",
    "    posted = datetime.now().strftime('%Y-%m-%d')\n",
    "    #skillsDesc = soup.find('section', 'description')\n",
    "    skillsDesc = soup.find('div', 'description__text description__text--rich')\n",
    "    edx = ''\n",
    "    if skillsDesc is not None: \n",
    "        skillsDesc = skillsDesc.get_text()\n",
    "    posting = Job_Posting(job, company, location, posted, edx, skillsDesc, False, False)\n",
    "    postings.append(posting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flipkart has a strong Data Science team in India that is working on cutting edge problems like multi-language speech recognition and natural language understanding, search and fraud detection. Flipkart has now established an AI Center in Redmond, Washington to advance the state of the art in Natural Language Understanding, Chatbots and Computer Vision. India’s eCommerce market is in its infancy and is growing very rapidly, and the level of complexity is both exhilarating and challenging. India is a country with its own version of English that is colorfully decorated with words such as chappal for slippers, and jhadu for a broom. Indian English is full of variety and complexity making it ripe for innovative natural language understanding. One of the areas of research will be natural language bots that can help with customer service and order fulfillment. The Seattle team will be responsible for developing natural language understanding technologies using state of the art Deep Learning techniques. The AI Center will also focus on Computer Vision applications that can be used for editorial purposes, for example identifying bad returns or identifying inappropriate images uploaded to the cloud. Each project will operate in a typical two-pizza team fashion with scientists, data engineers, product and program managers working to deliver state of the art solutions that will be integrated into Flipkart web front end and back end applications. Each team will have a single-threaded leader who will have wide latitude and independence. Basic Qualifications A Master’s degree in Computer Science, Math, Physics, Operations Research or equivalent discipline. 6+ years of experience (based on the level) Proven ability to ship NLP or Computer Vision applications Experience with deep learning frameworks such as Tensorflow, Keras, Pytorch Able to write production-level code, which is well-written and explainable Deep experience with Python Familiarity with data visualization tools Experience using ML libraries, such as scikit-learn Experience working with GPUs to develop models Experience handling terabyte size datasets Preferred Qualifications Ph.D. degree in Computer Science, Math, Physics or equivalent discipline Research publications in NLP or Computer Vision in peer-reviewed journals. Strong industry experience in developing practical AI applications Strong writing skills (writing sample is required as part of the interview process) Combination of business and technical savvy'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desc = postings[0].tech_skills\n",
    "desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description corpus i.e. each description is a document\n",
    "alltext = desc\n",
    "sk = ['python', 'pyspark', 'numpy', 'pandas', 'nltk', 'gensim', 'scikit-learn', 'sklearn', 'nlp', 'tensorflow',\n",
    "     'machine learning', 'AI', 'BI', 'ML', 'analysis', 'excel', 'vba', 'C#', 'Java', 'STATA', 'EVIEWS',\n",
    "     'deep learning', 'master\\'s', 'masters', 'bachelor\\'s', 'bachelors', 'models']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
