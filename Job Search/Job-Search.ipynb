{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "#!pip install requests -q\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime, timedelta\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create dictionaries of search parameters\n",
    "#Later on, add values to this as you get more job titles from search results and we can phase this out boo :)\n",
    "titles = [\n",
    "    'Data Scientist', 'Data', 'BI', 'Decision Science', 'ML', 'Machine Learning', 'Applied Scientist', \n",
    "    'Modeling Analyst', 'Business Analyst', 'Business Intelligence', 'Analyst',  'Developer', 'Scientist', \n",
    "    'Quantitative', 'Artificial Intelligence', 'AI', 'Engineer', 'Big-Data', 'Software Engineer'\n",
    "]\n",
    "size = len(titles)\n",
    "#search result job titles\n",
    "addon_titles = []\n",
    "\n",
    "qualifiers = ['Intern', 'Junior']\n",
    "for t in titles[:size]: \n",
    "    titles.extend([qualifiers[0]+'-'+ t, qualifiers[1]+'-' + t])\n",
    "    \n",
    "locationDict = {\n",
    "    0: 'Pittsburgh, PA',  1: 'Nashville, TN',  2: 'California',  3: 'Austin, TX',  4: 'Minneapolis, MN',  \n",
    "    5: 'San Antonio, TX'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "#description corpus i.e. each description is a document\n",
    "#alltext = desc\n",
    "sk = ['python', 'pyspark', 'numpy', 'pandas', 'nltk', 'gensim', 'scikit-learn', 'sklearn', 'nlp', 'tensorflow',\n",
    "     'machine learning', 'AI', 'BI', 'ML', 'analysis', 'excel', 'vba', 'C#', 'Java', 'STATA', 'EVIEWS',\n",
    "     'deep learning', 'master\\'s', 'masters', 'bachelor\\'s', 'bachelors', 'models']\n",
    "\n",
    "immigration = ['immigration', 'W2', 'sponsorship', 'visa', 'citizenship status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for each job posting\n",
    "class Job_Posting:\n",
    "    #attributes\n",
    "    \n",
    "    def __init__(self): pass\n",
    "    \n",
    "    def __init__(self, title, company, location, date, skills, visa, relo):\n",
    "        self.job_title = title\n",
    "        self.company_name = company\n",
    "        self.job_location = location\n",
    "        self.posted_date = date\n",
    "        self.tech_skills = self.get_skills_requested(skills)\n",
    "        self.immigration_assistance = visa\n",
    "        self.relocation_assistance = relo\n",
    "        \n",
    "    def get_skills_requested(self, description):\n",
    "        skills = []\n",
    "        #model, work in progress. Will move here once finalized for phase 1\n",
    "        return description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to get search results for dictionary positions and all locations before filtering\n",
    "#Posted in the last 24 hours and <= 10 miles from job location\n",
    "def get_all_location_results(job_titles, location): \n",
    "    end = len(job_titles)\n",
    "    links = []\n",
    "    print(end, 'search terms: \\n-----------------------------\\n')\n",
    "    for title in job_titles:\n",
    "        URL = 'https://www.linkedin.com/jobs/search?keywords='+ title +'&location='+location+'&f_TP=1'\n",
    "        page = requests.get(URL)\n",
    "        soup = bs(page.content, 'lxml')\n",
    "        refs = soup.find_all('a', class_='result-card__full-card-link')\n",
    "        links += [ref.get('href') for ref in refs if ref.get('href') not in links]\n",
    "    print(location + ':', len(links), 'result links')\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [\n",
    "    'Data Scientist', 'Data', 'BI', 'Decision Science', 'ML', 'Machine Learning', 'Applied Scientist', \n",
    "    'Modeling Analyst', 'Business Analyst', 'Business Intelligence', 'Analyst',  'Developer', 'Scientist', \n",
    "    'Quantitative', 'Artificial Intelligence', 'AI', 'Engineer', 'Big-Data', 'Software Engineer'\n",
    "]\n",
    "#method to get the info in the links\n",
    "def get_job_postings(links, titles):\n",
    "    end = len(links)\n",
    "    print('Number of links to start:', end)\n",
    "    postings = []\n",
    "    job_titles = []#new stuff\n",
    "    for link in links:\n",
    "        #get page data\n",
    "        page = requests.get(link)\n",
    "        soup = bs(page.content, 'lxml')\n",
    "        job = soup.find('h1', 'topcard__title')\n",
    "        if job is not None:\n",
    "            job = re.sub(r'[^\\w\\s]','', job.get_text().strip())\n",
    "            if job.title() not in titles: job_titles.append(job.title())\n",
    "            company = soup.find('a', 'topcard__org-name-link topcard__flavor--black-link')\n",
    "            if company is not None: company = company.get_text().strip()\n",
    "            location = soup.find('span', 'topcard__flavor topcard__flavor--bullet')\n",
    "            if location is not None: location = location.get_text().strip()\n",
    "            posted = datetime.now().strftime('%Y-%m-%d')\n",
    "            skillsDesc = soup.find('div', 'description__text description__text--rich')\n",
    "            if skillsDesc is not None: skillsDesc = skillsDesc.get_text()\n",
    "            #check to see for desired locations\n",
    "            loc_list = ' '.join(locationDict.values())\n",
    "            if loc_list.find(location) != -1:\n",
    "                posting = Job_Posting(job, company, location, posted, skillsDesc, False, False)\n",
    "                postings.append(posting)\n",
    "    print('Today\\'s new job titles:', len(job_titles))     \n",
    "    titles += job_titles\n",
    "    return postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to tokenize and pre-process job descriptions\n",
    "def tokenize_job_descriptions(postings):\n",
    "    word_freq = defaultdict(int)\n",
    "    descs = []\n",
    "    for posting in postings:\n",
    "        desc = posting.tech_skills\n",
    "        if desc is not None:\n",
    "            desc = desc.lower()\n",
    "            desc = re.sub(r'[^c#\\w\\s]', ' ', desc)\n",
    "            desc = desc.split()\n",
    "            desc = [word for word in desc if word not in stopwords.words('english')]\n",
    "            desc = [word for word in desc if len(word) > 1 or (len(word) == 1 and word =='r')]\n",
    "            desc.append(posting.job_title)\n",
    "            for word in desc:\n",
    "                word_freq[word] += 1\n",
    "            descs.append(desc)\n",
    "    print('Total unique tokens from list:', len(word_freq))\n",
    "    return descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19 search terms: \n",
      "-----------------------------\n",
      "\n",
      "United-States: 473 result links\n",
      "Time to get search results: 0:15 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "links = []\n",
    "links += get_all_location_results(titles, 'United-States')\n",
    "print('Time to get search results: {m}:{s:02d} mins'.format(m = round((time() - t )/ 60), s = round((time()-t)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links to start: 473\n",
      "Today's new job titles: 389\n",
      "Total job postings: 16\n",
      "Time to extract link info: 6:41 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "postings = get_job_postings(links, titles)\n",
    "print('Total job postings:', len(postings))\n",
    "print('Time to extract link info: {m}:{s:02d} mins'.format(m = round((time() - t )/ 60), s = round((time()-t)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens from list: 978\n",
      "Time to tokenize values: 0:06 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "descs = tokenize_job_descriptions(postings)\n",
    "print('Time to tokenize values: {m}:{s:02d} mins'.format(m = round((time() - t )/ 60), s = round((time()-t)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Links: 473 postings: 16\n",
      "descriptions: 16\n"
     ]
    }
   ],
   "source": [
    "print('Links:', len(links), 'postings:', len(postings))\n",
    "print('descriptions:',len(descs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Gensim Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim model\n",
    "#size: The number of dimensions of the embeddings and the default is 100.\n",
    "#window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "#min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "#workers: The number of partitions during training and the default workers is 3.\n",
    "#sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "model = Word2Vec(min_count=1, size= 50,workers=3, window =3, sg = 1, alpha=0.03, min_alpha=0.0007)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0:01 mins\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "model.build_vocab(descs, progress_per=10000)\n",
    "print('Time to build vocab: {m}:{s:02d} mins'.format(m = round((time() - t )/ 60), s = round((time()-t)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train model: 0:01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "size = int(len(descs)*0.8)\n",
    "model.train(descs[:size], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "print('Time to train model: {m}:{s:02d} mins'.format(m = round((time() - t )/ 60), s = round((time()-t)%60)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make model more memory efficient\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('java', 0.9938955307006836),\n",
       " ('scala', 0.991338849067688),\n",
       " ('one', 0.9663410186767578),\n",
       " ('equivalent', 0.9582749605178833),\n",
       " ('qualificationsmastery', 0.9481650590896606),\n",
       " ('sparkexperience', 0.9437363147735596),\n",
       " ('languageexperience', 0.9403271675109863),\n",
       " ('language', 0.9319831132888794),\n",
       " ('scalaexperience', 0.9282258749008179),\n",
       " ('libraries', 0.9249277114868164)]"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what words are most similar to this\n",
    "model.wv.most_similar(positive=['python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8321775"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how similar are these two\n",
    "model.wv.similarity('python', 'spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#odd one out\n",
    "model.wv.doesnt_match(['master', 'python', 'r'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('scala', 0.9402890205383301),\n",
       " ('qualificationsmastery', 0.9359278678894043),\n",
       " ('equivalent', 0.9343596696853638),\n",
       " ('java', 0.9301947355270386),\n",
       " ('one', 0.9272948503494263)]"
      ]
     },
     "execution_count": 630,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#master is to degree as python is to....?\n",
    "model.wv.most_similar(positive=['master', 'python'], negative=['degree'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF IDF with sk learn trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#modules needed\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "descs = []\n",
    "#Pre processing: remove punctuation and symbols, change all to lower case, tokenize\n",
    "for d in descriptions:\n",
    "    d = d.lower()\n",
    "    d = re.sub(r'[^\\w\\s]', ' ', d)\n",
    "    descs.append(d)\n",
    "\n",
    "#Create vectorizer for the descriptions\n",
    "vect = TfidfVectorizer(max_df = 0.7, min_df = 3, stop_words = stopwords.words('english'))\n",
    "tfidf = vect.fit_transform(descs)\n",
    "#for idx in range(tfIDF[0].shape[0]):\n",
    "  #  print(vect.get_feature_names()[idx], tfIDF[0][idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#(tfidf[0].toarray())\n",
    "words = list(vect.vocabulary_.keys()) #the feature names\n",
    "#use key values and keys to change the dataframe so that we can see the names of the words\n",
    "keys = list(vect.vocabulary_.values()) #the number representing the wordID in the tfidf vectors (docID, wordID)\n",
    "values = vect.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for word, key in values.items(): #every word\n",
    "    scores = scores.rename(columns = {key : word})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#row index = word keyID\n",
    "#column index = document ID\n",
    "scores = pd.DataFrame(tfidf.toarray())\n",
    "x = scores[3][0] #column index, row index (in that order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = scores.sum(axis=0) #all values tfidf sums\n",
    "b = scores.sum(axis=0).index #all words/index\n",
    "c = scores.sum(axis=0).values #all sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tomorrow Inshallah: create small methods to parse through a string to look for substring before saving job posting:\n",
    "- Only save posting if relo assistance is offered\n",
    "- Only save posting if immigration is offered\n",
    "- Send list of job posts to excel and download/add to it\n",
    "- add link of listing to the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes\n"
     ]
    }
   ],
   "source": [
    "s = 'data science'\n",
    "ss = 'data'\n",
    "if s.find(ss) != -1:\n",
    "    print('yes')\n",
    "else: print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pittsburgh Nashville California Austin Minneapolis San Antonio'"
      ]
     },
     "execution_count": 566,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_list = \n",
    "loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
