{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing filtering out sentences\n",
    "- Split a description into sentence tokens = **done!**\n",
    "- Parse through each sentence to see if it has any of the key words for immigration pointers = **done!**\n",
    "- If there is no flag word in the description, add a new description e.g. 'no immigration flag'or maybe we will make one that denotes nothing and say 'check company stance'  = **done!** \n",
    "- Use the top 100 H1B list iff the original pass through returns 'check company stance' = **done!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import job_postings as jp\n",
    "import job_description_features as jdf\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 search terms: \n",
      "--------------------------------\n",
      "\n",
      "United States: 350 result links for June 16, 2020\n",
      "Time to retrieve postings: 4:09 mins\n"
     ]
    }
   ],
   "source": [
    "t = time() #get today's postings (last 24 hrs), filter out links we dont want \n",
    "postings = jp.New_Postings()\n",
    "jp.print_time('Time to retrieve postings:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total remaining job postings: 290\n",
      "Time to create dataframe: 0:11 mins\n"
     ]
    }
   ],
   "source": [
    "t = time() #create dataframe with cleaned columns\n",
    "data = postings.get_job_postings()\n",
    "jp.print_time('Time to create dataframe:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa_yes = data[data['sponsor'] == 'Yes']\n",
    "visa_find = data[data['sponsor'] != 'Yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visa_yes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(277, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visa_find.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('output-today.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to remove stopwords, punctuation, lemmatize job descriptions\n",
    "from nltk import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "#method to get tokens (might switch to ngrams) from the cleaned description\n",
    "def process_job_descriptions(descs):\n",
    "    wn = WordNetLemmatizer()\n",
    "    for desc in descs:\n",
    "        if desc != 'check company stance':\n",
    "            desc = [wn.lemmatize(w) for w in desc if len(w) > 1 or (len(w) == 1 and w =='r')]\n",
    "            for word in desc:\n",
    "                word_freq[word] += 1\n",
    "    print('Total unique tokens from list:', len(word_freq))\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing Recursion \n",
    "May be helpful to save processing time for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meth(n):\n",
    "    if n == 0: return n\n",
    "    if n == 1: return n+n\n",
    "    return meth(n-1) + n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#synonyms = might help with getting equivalent of visa/sponsorship words \n",
    "#so that we dont store a large corpus and also look for synonyms for phrases\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_word_lemmas(word):\n",
    "    lemma_defs = []\n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "    syn = wordnet.synsets(word)\n",
    "    print('Word of Interest:', syn[0].lemmas()[0].name())\n",
    "    for s in syn: \n",
    "        lemma_defs.append((s.lemmas()[0].name(), s.definition()))\n",
    "    #trying to see lemmas?\n",
    "    for s in syn:\n",
    "        for l in s.lemmas():\n",
    "            if l.name() not in synonyms: synonyms.append(l.name()) \n",
    "            if l.antonyms(): \n",
    "                for ant in l.antonyms(): \n",
    "                    if ant.name() not in antonyms: antonyms.append(ant.name())\n",
    "    return [lemma_defs, synonyms, antonyms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsyns = get_word_lemmas('work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarities between words (scores between 0 and 1)\n",
    "w = wordnet.synset('work.n.01') #n = noun\n",
    "v = wordnet.synset('visa.n.01')\n",
    "a = wordnet.synset('authorization.n.01')\n",
    "v.wup_similarity(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descs = []\n",
    "for word in words:\n",
    "    x = defe.Description_Features(word)\n",
    "    descs.append(x.clean_description_text())\n",
    "descs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving copy of data in excel (if needed)\n",
    "We started this but maybe we can do a web display of the data instead with that streamlit thing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time()\n",
    "print('Links:', len(links), '\\tPostings:', len(postings))\n",
    "print('Descriptions:', len(descs))\n",
    "descs = [posting.description for posting in postings]\n",
    "#save to file\n",
    "import csv\n",
    "with open('Postings.csv', mode='w', encoding='UTF-8') as posts_file:\n",
    "    post_writer = csv.writer(posts_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL,\n",
    "                            lineterminator = '\\n')\n",
    "    post_writer.writerow(['Title', 'Location', 'Company', 'Link', 'Description'])\n",
    "    for posting in postings:\n",
    "        post_writer.writerow([posting.job_title, posting.job_location, posting.company_name,\\\n",
    "                              posting.job_link, posting.description])\n",
    "posts_file.close()\n",
    "print_time('Time to save job postings:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5,6]\n",
    "x[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-20-f4310e79e484>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-20-f4310e79e484>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    cut = len(self.postings) + 10\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    " start = 0\n",
    "        cut = len(self.postings) + 10\n",
    "        end = len(self.links)\n",
    "        while start <= end and cut <= end:\n",
    "            for link in self.links[start: cut]:\n",
    "                post = Posting(link=link)\n",
    "                if post: self.postings.append(post)\n",
    "            start = cut + 1\n",
    "            cut+= 10      \n",
    "            if cut > end: cut = end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Gensim Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modules\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "descs = data.desc_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gensim model\n",
    "#size: The number of dimensions of the embeddings and the default is 100.\n",
    "#window: The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "#min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\n",
    "#workers: The number of partitions during training and the default workers is 3.\n",
    "#sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "model = Word2Vec(min_count=1, size= 50,workers=3, window =3, sg = 0, alpha=0.03, min_alpha=0.0007)\n",
    "\n",
    "from time import time\n",
    "t = time()\n",
    "model.build_vocab(descs, progress_per=1000)\n",
    "jp.print_time('Time to build vocab:', t)\n",
    "t = time()\n",
    "size = int(len(descs)*0.8)\n",
    "\n",
    "model.train(descs[:size], total_examples=model.corpus_count, epochs=30, report_delay=1)\n",
    "jp.print_time('Time to train model:', t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make model more memory efficient\n",
    "model.init_sims(replace=True)\n",
    "#what words are most similar to this\n",
    "model.wv.most_similar(positive=['data'])\n",
    "#how similar are these two\n",
    "model.wv.similarity('python', 'spark')\n",
    "#odd one out\n",
    "model.wv.doesnt_match(['master', 'python', 'r'])\n",
    "#master is to degree as python is to....?\n",
    "model.wv.most_similar(positive=['master', 'python'], negative=['degree'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
