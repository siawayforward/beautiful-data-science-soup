{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job Postings Retrieval\n",
    "This script will contain the following classes and methods\n",
    "- class: New_Postings\n",
    "- class: Posting\n",
    "- method: print_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_postings.py\n"
     ]
    }
   ],
   "source": [
    "%%file job_postings.py\n",
    "\n",
    "#needed modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import defaultdict\n",
    "from datetime import datetime, timedelta\n",
    "from nltk.corpus import stopwords\n",
    "from time import time\n",
    "import job_description_features as jdf\n",
    "\n",
    "#class for each job posting\n",
    "class Posting:        \n",
    "    def __init__(self, link):\n",
    "        self.job_link = link\n",
    "        #get details and assign to attributes\n",
    "        details = self.get_job_posting_info()\n",
    "        if details:\n",
    "            self.job_title = details.get('title')\n",
    "            self.company_name = details.get('company')\n",
    "            self.job_location = details.get('location')\n",
    "            self.description = details.get('desc')\n",
    "        else: self = None\n",
    "    \n",
    "    #extract desired skills from description\n",
    "    def tech_skills(self, desc = None):\n",
    "        #model, work in progress. Will move here once finalized for phase 1 \n",
    "        if desc: #do stuff\n",
    "            self._tech_skills = desc\n",
    "        #return self._tech_skills\n",
    "    \n",
    "    #extract citizenship stance of organization or for position from description\n",
    "    def allows_immigrants(self, desc = None):\n",
    "        #model, work in progress. Will move here once finalized for phase 2        \n",
    "        #flag = True #do stuff here\n",
    "        self._immigration_assistance = flag\n",
    "        return self._immigration_assistance\n",
    "    \n",
    "    #extract relocation assistance stance of organization or for position from description\n",
    "    def has_relocation_help(self, desc = None):\n",
    "        #model, work in progress. Will move here once finalized for phase 2        \n",
    "        #flag = True #do stuff here\n",
    "        self._relocation_assistance = flag\n",
    "        return self._relocation_assistance\n",
    "\n",
    "    #method to get the info in the links in order\n",
    "    def get_job_posting_info(self):      \n",
    "        #get page data\n",
    "        page = requests.get(self.job_link)\n",
    "        soup = bs(page.content, 'lxml')\n",
    "        #get value fields\n",
    "        job = soup.find('h1', 'topcard__title')\n",
    "        location = soup.find('span', 'topcard__flavor topcard__flavor--bullet')\n",
    "        company = soup.find('a', 'topcard__org-name-link topcard__flavor--black-link')\n",
    "        desc = soup.find('div', 'description__text description__text--rich')\n",
    "        if job is not None and location is not None and company is not None and desc is not None:\n",
    "            #add job attributes if verified\n",
    "            job = job.get_text().strip()\n",
    "            location = location.get_text().strip()\n",
    "            company = company.get_text().strip()\n",
    "            desc = desc.get_text()\n",
    "            details = {'title': job, 'location': location, 'company': company, 'desc': desc}\n",
    "            return details\n",
    "        else: return None\n",
    "            \n",
    "#class to find, filter, and process job postings\n",
    "class New_Postings():\n",
    "    #jobs to search\n",
    "    job_titles = pd.read_csv('titles.txt', header=None)[0].values.tolist()\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.today = datetime.now().strftime('%B %d, %Y')\n",
    "        self.links = self.get_all_location_results()\n",
    "        self.postings = []\n",
    "        for link in self.links:\n",
    "            post = Posting(link=link)\n",
    "            if post: self.postings.append(post)\n",
    "        self.postings = list(filter(self.filter_title_and_location, self.postings))\n",
    "        \n",
    "    #method to get search results for dictionary positions and all locations before filtering\n",
    "    #Posted in the last 24 hours and <= 10 miles from job location\n",
    "    def get_all_location_results(self, location = 'United States'): \n",
    "        end = len(self.job_titles)\n",
    "        self.links = []\n",
    "        print(end, 'search terms: \\n--------------------------------\\n')\n",
    "        for title in self.job_titles:\n",
    "            URL = 'https://www.linkedin.com/jobs/search?keywords='+ title +'&location='+location+'&f_TP=1'\n",
    "            page = requests.get(URL)\n",
    "            soup = bs(page.content, 'lxml')\n",
    "            refs = soup.find_all('a', class_='result-card__full-card-link')\n",
    "            self.links += [ref.get('href') for ref in refs if ref.get('href') not in self.links]\n",
    "        print(location + ':', len(self.links), 'result links for', self.today)\n",
    "        return self.links\n",
    "    \n",
    "    #method to check whether title is valid for entry, associate, internship level, non-government job\n",
    "    def filter_title_and_location(self, job): #true is the thing we want to keep\n",
    "        filters = ['VP', 'manager', 'senior', 'sr', 'president', 'vice president', 'director']\n",
    "        #check if title has senior tags/location is Virginia = government jobs/need clearance+residence\n",
    "        try:\n",
    "            for w in filters:\n",
    "                if job.job_title.upper().find(w.upper()) != -1: return False\n",
    "            if job.job_location.upper().find(', VA') != -1: return False\n",
    "            else: return True\n",
    "        except: pass\n",
    "        \n",
    "    #method to return the processed job postings as a dataframe\n",
    "    def get_job_postings(self):\n",
    "        #filter out non-qualified jobs\n",
    "        print('Total remaining job postings:', len(self.postings))\n",
    "        companies = jdf.get_H1B_approvers()\n",
    "        jobs = []\n",
    "        for p in self.postings:\n",
    "            val = jdf.Description_Features(p.description)\n",
    "            jobs.append({'title': p.job_title, 'company': p.company_name, \n",
    "                         'location': p.job_location, 'desc_raw': p.description,\n",
    "                        'desc_visa': val.clean_description_text(),\n",
    "                        'sponsor':val.get_immigration_stance(p.company_name, companies)})\n",
    "        self.job_data = pd.DataFrame(jobs)\n",
    "        return self.job_data\n",
    "    \n",
    "#time printer\n",
    "def print_time(note, t):\n",
    "    print(note, '{m}:{s:02d} mins'\\\n",
    "          .format(m = round((time() - t )/ 60), s = round((time()-t)%60)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting Description Processing\n",
    "This script will contain the following classes and methods for processing the description text of each job posting\n",
    "- class: Description_Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting job_description_features.py\n"
     ]
    }
   ],
   "source": [
    "%%file job_description_features.py\n",
    "\n",
    "#modules needed\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "import wordninja\n",
    "\n",
    "#class to preprocess the description and get target description for feature extraction\n",
    "class Description_Features():    \n",
    "    #tags to filter description with for immigration\n",
    "    immigration_tags = ['security clearance', 'citizens', 'citizen', 'green card', \n",
    "                             'authorized', 'authorization', 'sponsorship', 'visa', 'US citizen',  \n",
    "                             'eligible',  'TS/SCI',  'DoD', 'secret clearance', 'resident', 'W2'\n",
    "                             'US persons', 'equal employment', 'EEO', 'citizenship', 'immigration',\n",
    "                             'citizenship status', 'No C2C', 'W2 only', 'visas', 'clearance'\n",
    "                            ]\n",
    "    wn = WordNetLemmatizer()\n",
    "    \n",
    "    def __init__(self, text = None):\n",
    "        if text: self.description = text\n",
    "        self.immigration_tags = [tag.lower() for tag in self.immigration_tags]\n",
    "    \n",
    "    #allows you to parse through text to see if some words are attached by mistake and add a space\n",
    "    def parsing_description(self):\n",
    "        desc = []\n",
    "        for word in self.description.split():\n",
    "            if len(word) <= 8: desc.append(word)\n",
    "            else: desc.append(' '.join(wordninja.split(word)))\n",
    "        self.description = ' '.join(desc)\n",
    "        return self.description\n",
    "    \n",
    "    def filter_tags(self, desc):\n",
    "        check = 0\n",
    "        for tag in self.immigration_tags:\n",
    "            if desc.lower().find(tag) != -1: check += 1 #tag found\n",
    "        if check == 0: return False\n",
    "        else: return True\n",
    "\n",
    "    def target_description(self):\n",
    "        #get sentence tokens from semi-cleaned description and filter out ones with no target tags\n",
    "        sents = sent_tokenize(self.description)\n",
    "        sents = list(filter(self.filter_tags, sents))\n",
    "        if len(sents) == 0: sents.append('check company stance')\n",
    "        self.filtered_description = ' '.join(sents)\n",
    "        return self.filtered_description\n",
    "    \n",
    "    #ML pipeline to parse description, tokenize, lower case, get target lemmatized tokens (joined)\n",
    "    def clean_description_text(self):\n",
    "        #cleaning a description\n",
    "        #remove/ignore non-ASCII characters and years\n",
    "        self.description = re.sub(r'[^\\x00-\\x7f]', '', self.description) \n",
    "        self.description = re.sub(r'[0-9]{3,}', '', self.description)\n",
    "        #make sure all words are displayed correctly/no words attached by mistake, remove stopwords\n",
    "        desc = self.parsing_description()\n",
    "        stopwords = list(stopwords.words('english'))\n",
    "        stops = [stop for stop in stopwords.words('english') if stop not in ['no', 'not', 'only']]\n",
    "        self.description = ' '.join([w.lower() for w in desc.split() if w.lower() not in stops])\n",
    "\n",
    "        #only get sentences with immigration indicator words/phrases (requires sent_tokenization)\n",
    "        filter_desc = self.target_description()\n",
    "        #remove unwanted/non-context punctuation marks after un-tokenizing sentences\n",
    "        filters = ''.join([x for x in string.punctuation if x != '#' and x != '+'])\n",
    "        desc = ''.join([char for char in filter_desc if char not in filters])\n",
    "        self.filtered_description = ' '.join([wn.lemmatize(word) for word in word_tokenize(desc)])\n",
    "        return self.filtered_description\n",
    "    \n",
    "    #lemmatize the descriptions and create tokens\n",
    "    def tokenize(self):\n",
    "        if self.filtered_description == 'check company stance': self.tokens = ''\n",
    "        else: self.tokens = [wn.lemmatize(word) for word in word_tokenize(desc_visa)]\n",
    "        return self.tokens\n",
    "    \n",
    "    #method to check immigration stance based on description or company (from H1B list)\n",
    "    def get_immigration_stance(self, comp, companies_list):\n",
    "        self.immigration = 'Unknown'\n",
    "        if self.filtered_description == 'check company stance':\n",
    "            for org in companies_list:\n",
    "                flag = [] #tracking companies names from H1B list\n",
    "                if comp.lower() in org.lower() or comp.lower() in org.lower(): flag.append(True)\n",
    "                if True in flag: self.immigration = 'Yes'\n",
    "        return self.immigration                    \n",
    "    \n",
    "    def collocation_finder(self, n_gram_total, n_gram_filter_word):\n",
    "        cf = TrigramCollocationFinder.from_words(self.sentence_tokens[0].split()) \n",
    "        #checking what words appear frequently with 'word' in this case it is 'work'\n",
    "        n_filter = lambda *words: n_gram_filter_word not in words\n",
    "        cf.apply_ngram_filter(n_filter)\n",
    "        #apply frq filter removes occurences that happened less than x times\n",
    "        self.collocation_scores = cf.nbest(TrigramAssocMeasures.likelihood_ratio, n_gram_total)\n",
    "        return self.collocation_scores\n",
    "    \n",
    "#method to get the company names for top H1B approved companies for the current year\n",
    "def get_H1B_approvers(pages = range(1,5)):\n",
    "    h1b_companies = []\n",
    "    for page in pages:\n",
    "        URL = 'http://www.myvisajobs.com/Reports/2020-H1B-Visa-Sponsor.aspx?P=' + str(page)\n",
    "        page = requests.get(URL)\n",
    "        soup = bs(page.content, 'lxml')\n",
    "        table = soup.find('table', class_='tbl').find_all('tr')[1:] #minus header row\n",
    "        for tr in table: \n",
    "            try: h1b_companies.append(tr.find_all('td')[1].text)\n",
    "            except: pass\n",
    "    return h1b_companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
