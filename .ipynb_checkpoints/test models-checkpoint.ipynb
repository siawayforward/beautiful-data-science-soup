{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing out model generation\n",
    "Use the cleaned out description to create n-grams/lemmatize/tfidf to get information about the features in the values\n",
    "- Use LDA or ngram frequency to get the immigration markers\n",
    "- Use collocation finder to get the years/experience markers*\n",
    "- Create a model with tfidf to see if the job provides immigration or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import job_description_features as jdf\n",
    "import job_postings as jp\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('output-today.csv')\n",
    "wn = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>desc_raw</th>\n",
       "      <th>desc_visa</th>\n",
       "      <th>sponsor</th>\n",
       "      <th>desc_visa_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Warner Bros. Entertainment</td>\n",
       "      <td>Burbank, CA</td>\n",
       "      <td>The JobWarner Bros. Entertainment Inc. seeks a...</td>\n",
       "      <td>check company stance</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientist (Level 1)</td>\n",
       "      <td>Arima Genomics, Inc.</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>Role: Scientist I – Product DevelopmentCompany...</td>\n",
       "      <td>role scientist product development company des...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[role, scientist, product, development, compan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>CGG</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>Company DescriptionCGG is a fully integrated G...</td>\n",
       "      <td>process seismic data peta scale warehouse info...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>[process, seismic, data, peta, scale, warehous...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Prudent Technologies and Consulting, Inc.</td>\n",
       "      <td>Grapevine, TX</td>\n",
       "      <td>Position Title: Data Scientist Location:  G...</td>\n",
       "      <td>check company stance</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Seismic</td>\n",
       "      <td>San Diego, CA</td>\n",
       "      <td>DescriptionAbout Seismic Data Team:At Seismic,...</td>\n",
       "      <td>check company stance</td>\n",
       "      <td>Unknown</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 title                                    company  \\\n",
       "0       Data Scientist                 Warner Bros. Entertainment   \n",
       "1  Scientist (Level 1)                       Arima Genomics, Inc.   \n",
       "2       Data Scientist                                        CGG   \n",
       "3       Data Scientist  Prudent Technologies and Consulting, Inc.   \n",
       "4       Data Scientist                                    Seismic   \n",
       "\n",
       "        location                                           desc_raw  \\\n",
       "0    Burbank, CA  The JobWarner Bros. Entertainment Inc. seeks a...   \n",
       "1  San Diego, CA  Role: Scientist I – Product DevelopmentCompany...   \n",
       "2    Houston, TX  Company DescriptionCGG is a fully integrated G...   \n",
       "3  Grapevine, TX     Position Title: Data Scientist Location:  G...   \n",
       "4  San Diego, CA  DescriptionAbout Seismic Data Team:At Seismic,...   \n",
       "\n",
       "                                           desc_visa  sponsor  \\\n",
       "0                               check company stance  Unknown   \n",
       "1  role scientist product development company des...  Unknown   \n",
       "2  process seismic data peta scale warehouse info...  Unknown   \n",
       "3                               check company stance  Unknown   \n",
       "4                               check company stance  Unknown   \n",
       "\n",
       "                                    desc_visa_tokens  \n",
       "0                                                     \n",
       "1  [role, scientist, product, development, compan...  \n",
       "2  [process, seismic, data, peta, scale, warehous...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['desc_visa_tokens'] = data['desc_visa'].apply(tokenize)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the collocation finder \n",
    "- So that we can use it to see what are the common phrases. We will then decide whether or not to employ n-grams and cluster them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "def tokenize(text):\n",
    "        if text == 'check company stance': tokens = ''\n",
    "        else: tokens = [wn.lemmatize(word) for word in word_tokenize(text)]\n",
    "        return tokens\n",
    "    \n",
    "def collocation_finder(text, n_gram_total, n_gram_filter_word):\n",
    "        cf = TrigramCollocationFinder.from_words(tokenize(text)) \n",
    "        #checking what words appear frequently with 'word' in this case it is 'work'\n",
    "        n_filter = lambda *words: n_gram_filter_word not in words\n",
    "        cf.apply_ngram_filter(n_filter)\n",
    "        #apply frq filter removes occurences that happened less than x times\n",
    "        collocation_scores = cf.nbest(TrigramAssocMeasures.likelihood_ratio, n_gram_total)\n",
    "        return collocation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(data.desc_visa[data['desc_visa_tokens'] != ''])\n",
    "for text in texts:\n",
    "    grams = collocation_finder(text, 5, 'not')\n",
    "    #if len(grams) > 0:   print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tfidf = TfidfVectorizer(ngram_range=(3,3))\n",
    "vectorizer_ct = CountVectorizer(ngram_range=(3,6))\n",
    "data_tfidf = vectorizer_ct.fit_transform(\n",
    "    data.desc_visa[data['desc_visa'] != 'check company stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=2, n_iter=100,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=2, n_iter=100)\n",
    "lsa.fit(data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.02936346e-09,  1.02936325e-09,  1.02936332e-09, ...,\n",
       "       -5.16280175e-19, -5.16280175e-19, -5.16280175e-19])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concept 0\n",
      "business analyst ii\n",
      "description business analyst\n",
      "problem solving skills\n",
      "ici enc ies\n",
      "prof ici enc\n",
      "prof ici enc ies\n",
      "various business units\n",
      "ability act effective\n",
      "ability act effective leader\n",
      "ability act effective leader among\n",
      "ability act effective leader among business\n",
      "ability authoring analyzing\n",
      "ability authoring analyzing process\n",
      "ability authoring analyzing process flow\n",
      "ability authoring analyzing process flow diagrams\n",
      "\n",
      "Concept 1\n",
      "us citizenship required\n",
      "national security space\n",
      "national security space programs\n",
      "security space programs\n",
      "apache mxnet tensor\n",
      "apache mxnet tensor flow\n",
      "apache mxnet tensor flow caffe2\n",
      "apache mxnet tensor flow caffe2 keras\n",
      "caffe2 keras microsoft\n",
      "caffe2 keras microsoft cognitive\n",
      "caffe2 keras microsoft cognitive toolkit\n",
      "caffe2 keras microsoft cognitive toolkit torch\n",
      "cognitive toolkit torch\n",
      "cognitive toolkit torch no\n",
      "deep learning frameworks\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer_ct.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sortedTerms = sorted(terms_comp,key = lambda x: x[1], reverse=True) [:15] #first ten items\n",
    "    print('\\nConcept {}'.format(i))\n",
    "    for term in sortedTerms: print(term[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters /trying to see if there is an unsupervised thingy going\n",
    "from sklearn.cluster import KMeans\n",
    "true_k = 2\n",
    "model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1, n_jobs=-1)\n",
    "model.fit(data_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster {}:\".format(i)),\n",
    "    terms = [terms[ind] for ind in order_centroids[i, :20]]\n",
    "    print(\"Terms:\", terms)\n",
    "    #for ind in order_centroids[i, :30]:\n",
    "        #print('{}'.format(terms[ind]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA topic/feature extraction\n",
    "May help us find common traits in text so that we can classify the descriptions based on topics of interest (immigration, health industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_A = list(data.desc_visa_tokens[data['desc_visa_tokens']!=''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LDA_Topic_Model():\n",
    "    def __init__(self):\n",
    "        self.model = Pipeline([\n",
    "            ('vect', TfidfVectorizer()),\n",
    "            ('model', LatentDirichletAllocation(n_components=2, n_jobs=-1)),\n",
    "        ])\n",
    "        \n",
    "    def fit_transform(self, documents):\n",
    "        self.model.fit_transform(documents)\n",
    "        return self.model\n",
    "    \n",
    "    def get_topics(self, n = 25):\n",
    "        vectorizer = self.model.named_steps['vect']\n",
    "        model = self.model.steps[-1][1]\n",
    "        model.n_jobs = -1\n",
    "        names = vectorizer.get_feature_names()\n",
    "        topics = dict()\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            features = topic.argsort()[:-(n - 1): -1]\n",
    "            tokens = [names[i] for i in features]\n",
    "            topics[idx] = tokens\n",
    "        return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "['data', 'status', 'experience', 'work', 'equal', 'opportunity', 'protected', 'employment', 'disability', 'applicant', 'information', 'team', 'national', 'employer', 'without', 'veteran', 'business', 'must', 'origin', 'knowledge', 'race', 'orientation', 'gender']\n",
      "Topic #2:\n",
      "['job', 'business', 'science', 'experience', 'degree', 'ability', 'skill', 'practice', 'work', 'required', 'computer', 'field', 'robert', 'opening', 'half', 'authorized', 'company', 'not', 'mathematics', 'related', 'solution', 'problem', 'eligible']\n",
      "Time to process: 0:01\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "if __name__ == '__main__':\n",
    "    documents = corpus_A\n",
    "    lda = LDA_Topic_Model()\n",
    "    lda.fit_transform(documents)\n",
    "    topics = lda.get_topics()\n",
    "    for topic, terms in topics.items():\n",
    "        print(\"Topic #{}:\".format(topic+1))\n",
    "        print(terms)\n",
    "print('Time to process: {}:{:02d}'.format(round((time() - t)/60), round((time() - t)%60)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
